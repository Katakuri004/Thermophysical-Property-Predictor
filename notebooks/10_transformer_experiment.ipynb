{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer Experiment: ChemBERTa Embeddings\n",
                "\n",
                "## 1. Overview\n",
                "We use a pre-trained Transformer model (`seyonec/ChemBERTa-zinc-base-v1`) to extract dense vector representations (embeddings) from SMILES strings. These embeddings capture deep chemical context learned from millions of molecules.\n",
                "\n",
                "We then train a regressor (XGBoost) on these embeddings to predict melting point."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import KFold\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath('..'))\n",
                "\n",
                "from src.features import ChemBERTaFeaturizer\n",
                "from src.models import XGBoostModel\n",
                "from src.utils.metrics import calculate_metrics\n",
                "\n",
                "# Set plots style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data and Generate Embeddings\n",
                "This step effectively replaces manual feature engineering with deep learning feature extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing ChemBERTa (this downloads the model if first time)...\n",
                        "Loading ChemBERTa model: seyonec/ChemBERTa-zinc-base-v1 on cuda...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "067a14b089be40da84a05f76c9036a32",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Kata\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kata\\.cache\\huggingface\\hub\\models--seyonec--ChemBERTa-zinc-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
                        "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
                        "  warnings.warn(message)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d449ea89f9474c4f9e46fcda6356e431",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5262d1073c7940bc800e1671e8d892d3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0700b8b1124b47f1a994ef221cb5db58",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9a5302d96cca4ead8ada338caea12eb6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
                        "2026-01-12 08:33:07,583 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "57c181c7394541dbb01a1c6332c85b7e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/179M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating embeddings for Train set...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Generating Embeddings:  58%|█████▊    | 49/84 [00:00<00:00, 66.85it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
                        "2026-01-12 08:33:39,546 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
                        "Generating Embeddings:  86%|████████▌ | 72/84 [00:01<00:00, 68.62it/s]"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cd93311b28bb4a89b6bd07bd681a7b5c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Generating Embeddings: 100%|██████████| 84/84 [00:01<00:00, 56.22it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating embeddings for Test set...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Generating Embeddings: 100%|██████████| 21/21 [00:00<00:00, 72.56it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Shapes: (2662, 1195) (666, 1194)\n",
                        "     id                       SMILES      Tm  Group 1  Group 2  Group 3  \\\n",
                        "0  2175        FC1=C(F)C(F)(F)C1(F)F  213.15        0        0        0   \n",
                        "1  1222  c1ccc2c(c1)ccc3Nc4ccccc4c23  407.15        0        0        0   \n",
                        "2  2994          CCN1C(C)=Nc2ccccc12  324.15        2        1        0   \n",
                        "3  1704                   CC#CC(=O)O  351.15        1        0        0   \n",
                        "4  2526                    CCCCC(S)C  126.15        2        3        0   \n",
                        "\n",
                        "   Group 4  Group 5  Group 6  Group 7  ...  ChemBERTa_758  ChemBERTa_759  \\\n",
                        "0        0        0        0        0  ...       0.516725      -0.220990   \n",
                        "1        0        0        0        0  ...       1.209761       0.328331   \n",
                        "2        0        0        0        0  ...       0.677359       0.531777   \n",
                        "3        0        0        0        0  ...       0.363788       0.355860   \n",
                        "4        0        0        0        0  ...      -0.732704      -0.026011   \n",
                        "\n",
                        "   ChemBERTa_760  ChemBERTa_761  ChemBERTa_762  ChemBERTa_763  ChemBERTa_764  \\\n",
                        "0      -0.162940      -0.439853      -0.608778      -0.019556      -1.131685   \n",
                        "1       1.397251       0.154326      -1.103747      -0.990460      -1.051306   \n",
                        "2       0.382764       1.456996       0.553349       0.727858      -1.672174   \n",
                        "3       0.226732      -2.036640       1.932053       0.176189      -0.711466   \n",
                        "4       0.508677      -1.302017      -0.627196      -1.498898      -1.638240   \n",
                        "\n",
                        "   ChemBERTa_765  ChemBERTa_766  ChemBERTa_767  \n",
                        "0      -1.320825      -0.354633       2.692831  \n",
                        "1      -1.318952       0.311973       0.087708  \n",
                        "2      -1.600629      -1.035165       1.821327  \n",
                        "3      -0.664907      -0.007503       1.236317  \n",
                        "4      -2.209779      -0.432889       2.097059  \n",
                        "\n",
                        "[5 rows x 1195 columns]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "train_raw = pd.read_csv('../data/raw/train.csv')\n",
                "test_raw = pd.read_csv('../data/raw/test.csv')\n",
                "\n",
                "print(\"Initializing ChemBERTa (this downloads the model if first time)...\")\n",
                "featurizer = ChemBERTaFeaturizer()\n",
                "\n",
                "print(\"Generating embeddings for Train set...\")\n",
                "train_emb = featurizer.calculate_transformer_features(train_raw, smiles_col='SMILES')\n",
                "\n",
                "print(\"Generating embeddings for Test set...\")\n",
                "test_emb = featurizer.calculate_transformer_features(test_raw, smiles_col='SMILES')\n",
                "\n",
                "print(\"Shapes:\", train_emb.shape, test_emb.shape)\n",
                "print(train_emb.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train XGBoost on Embeddings\n",
                "Embeddings are high-dimensional (768 dimensions), so XGBoost is a good choice to find non-linear patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training XGBoost on ChemBERTa embeddings...\n",
                        "Fold 1 MAE: 42.6619\n",
                        "Fold 2 MAE: 41.0009\n",
                        "Fold 3 MAE: 39.8200\n",
                        "Fold 4 MAE: 40.9981\n",
                        "Fold 5 MAE: 40.3526\n",
                        "\n",
                        "Average CV MAE (Transformer): 40.9667\n",
                        "Saved ChemBERTa submission.\n"
                    ]
                }
            ],
            "source": [
                "feature_cols = [c for c in train_emb.columns if c.startswith('ChemBERTa_')]\n",
                "X = train_emb[feature_cols]\n",
                "y = train_emb['Tm']\n",
                "X_test = test_emb[feature_cols]\n",
                "\n",
                "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "results = []\n",
                "test_fold_preds = []\n",
                "\n",
                "print(\"Training XGBoost on ChemBERTa embeddings...\")\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
                "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
                "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
                "    \n",
                "    # Embeddings are dense, might need slightly different params\n",
                "    model = XGBoostModel({'n_estimators': 2000, 'learning_rate': 0.02, 'max_depth': 6})\n",
                "    model.fit(X_train, y_train, X_val, y_val)\n",
                "    \n",
                "    val_pred = model.predict(X_val)\n",
                "    metrics = calculate_metrics(y_val, val_pred)\n",
                "    results.append(metrics)\n",
                "    print(f\"Fold {fold+1} MAE: {metrics['MAE']:.4f}\")\n",
                "    \n",
                "    test_fold_preds.append(model.predict(X_test))\n",
                "\n",
                "avg_mae = np.mean([m['MAE'] for m in results])\n",
                "print(f\"\\nAverage CV MAE (Transformer): {avg_mae:.4f}\")\n",
                "\n",
                "# Create Submission\n",
                "avg_preds = np.mean(test_fold_preds, axis=0)\n",
                "submission = pd.DataFrame({'id': test_emb['id'], 'Tm': avg_preds})\n",
                "submission.to_csv('../submissions/submission_chemberta.csv', index=False)\n",
                "print(\"Saved ChemBERTa submission.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
