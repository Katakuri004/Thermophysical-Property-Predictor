{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Massive Training: Incorporating External Datasets\n",
                "\n",
                "## 1. Objective\n",
                "We are boosting our training set from **~2,500** molecules to **~300,000+** molecules using external datasets.\n",
                "\n",
                "**Datasets:**\n",
                "1.  **Original Kaggle Train**: ~2.5k rows.\n",
                "2.  **Bradley Melting Point**: ~30k curated rows.\n",
                "3.  **SMILES Melting Point**: ~275k rows.\n",
                "\n",
                "**Strategy:**\n",
                "- Merge all datasets.\n",
                "- Clean and De-duplicate based on canonical SMILES.\n",
                "- Featurize using our `AdvancedMolecularFeaturizer`.\n",
                "- Train LightGBM with MAE optimization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from rdkit import Chem\n",
                "import lightgbm as lgb\n",
                "from lightgbm import LGBMRegressor\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "import sys\n",
                "import os\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath('..'))\n",
                "\n",
                "from src.features import AdvancedMolecularFeaturizer\n",
                "\n",
                "# Canonicalization helper\n",
                "def canonicalize(smiles):\n",
                "    try:\n",
                "        mol = Chem.MolFromSmiles(smiles)\n",
                "        if mol:\n",
                "            return Chem.MolToSmiles(mol, canonical=True)\n",
                "    except:\n",
                "        pass\n",
                "    return None\n",
                "\n",
                "print(\"Libraries Loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Merge Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Original Train\n",
                "df_train = pd.read_csv('../data/raw/train.csv')[['SMILES', 'Tm']]\n",
                "print(f\"Original Train: {df_train.shape}\")\n",
                "\n",
                "# 2. Bradley Datasets\n",
                "try:\n",
                "    df_bradley = pd.read_excel('../data/raw/BradleyMeltingPointDataset.xlsx')\n",
                "    df_bradleyplus = pd.read_excel('../data/raw/BradleyDoublePlusGoodMeltingPointDataset.xlsx')\n",
                "    \n",
                "    # Standardize\n",
                "    df_bradley['Tm'] = df_bradley['mpC'] + 273.15\n",
                "    df_bradleyplus['Tm'] = df_bradleyplus['mpC'] + 273.15\n",
                "    \n",
                "    df_bradley = df_bradley[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
                "    df_bradleyplus = df_bradleyplus[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
                "    \n",
                "    df_b_all = pd.concat([df_bradley, df_bradleyplus], axis=0)\n",
                "    print(f\"Bradley Combined: {df_b_all.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading Bradley data: {e}\")\n",
                "    df_b_all = pd.DataFrame(columns=['SMILES', 'Tm'])\n",
                "\n",
                "# 3. SMILES Melting Point\n",
                "try:\n",
                "    df_smiles_mp = pd.read_csv('../data/raw/smiles_melting_point.csv', on_bad_lines='skip')\n",
                "    # Attempt to handle different column names if they exist\n",
                "    col_map = {'Melting Point {measured, converted}': 'Tm', 'SMILES': 'SMILES'}\n",
                "    df_smiles_mp = df_smiles_mp.rename(columns=col_map)[['SMILES', 'Tm']]\n",
                "    print(f\"SMILES MP Dataset: {df_smiles_mp.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading SMILES MP data: {e}\")\n",
                "    df_smiles_mp = pd.DataFrame(columns=['SMILES', 'Tm'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge All\n",
                "full_train = pd.concat([df_train, df_b_all, df_smiles_mp], axis=0)\n",
                "print(f\"Total Raw Rows: {full_train.shape[0]}\")\n",
                "\n",
                "# Canonicalize and Deduplicate\n",
                "print(\"Canonicalizing SMILES (this takes time)...\")\n",
                "full_train['SMILES'] = full_train['SMILES'].apply(canonicalize)\n",
                "\n",
                "# Drop invalid SMILES\n",
                "full_train = full_train.dropna(subset=['SMILES'])\n",
                "\n",
                "# Clean duplicates (Keep last - usually means keep original train if we laid it out last, \n",
                "# but actually we put df_train first. Let's ensure high quality data takes precedence if we want)\n",
                "# A simple strategy: drop duplicates\n",
                "full_train = full_train.drop_duplicates(subset=['SMILES'], keep='first')\n",
                "\n",
                "print(f\"Final Unique Molecules: {full_train.shape[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Featurization\n",
                "Converting 300k molecules to features. This is heavy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter down for testing speed? UNCOMMENT TO SAMPLE if too slow\n",
                "# full_train = full_train.sample(50000, random_state=42)\n",
                "# print(\"Sampled down to 50k for feasibility.\")\n",
                "\n",
                "featurizer = AdvancedMolecularFeaturizer()\n",
                "\n",
                "print(\"Featurizing Full Train Set...\")\n",
                "# This generates a lot of columns. \n",
                "train_feats = featurizer.generate_features(full_train, smiles_col='SMILES')\n",
                "\n",
                "# Load Test and Featurize\n",
                "test_raw = pd.read_csv('../data/raw/test.csv')\n",
                "print(\"Featurizing Test Set...\")\n",
                "test_feats = featurizer.generate_features(test_raw, smiles_col='SMILES')\n",
                "\n",
                "# Ensure columns align\n",
                "train_feats = train_feats.drop(['SMILES'], axis=1)\n",
                "X = train_feats.drop(['Tm'], axis=1)\n",
                "y = train_feats['Tm']\n",
                "\n",
                "X_test = test_feats.drop(['id', 'SMILES'], axis=1)\n",
                "\n",
                "# Align columns (in case some features generated in train but not test due to lack of substructures)\n",
                "missing_cols = set(X.columns) - set(X_test.columns)\n",
                "for c in missing_cols:\n",
                "    X_test[c] = 0\n",
                "    \n",
                "extra_cols = set(X_test.columns) - set(X.columns)\n",
                "X_test = X_test.drop(list(extra_cols), axis=1)\n",
                "\n",
                "# Reorder\n",
                "X_test = X_test[X.columns]\n",
                "\n",
                "print(\"Final X Shape:\", X.shape)\n",
                "print(\"Final X_test Shape:\", X_test.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train LightGBM\n",
                "Using reference params."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    'n_estimators': 3000,\n",
                "    'learning_rate': 0.1,\n",
                "    'num_leaves': 100,\n",
                "    'max_depth': 15,\n",
                "    'min_child_samples': 50,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'random_state': 42,\n",
                "    'verbose': -1,\n",
                "    'objective': 'regression_l1', # MAE\n",
                "    'metric': 'mae'\n",
                "}\n",
                "\n",
                "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "results = []\n",
                "final_preds = np.zeros(len(X_test))\n",
                "\n",
                "print(\"Training LightGBM on Massive Dataset...\")\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
                "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
                "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
                "    \n",
                "    model = LGBMRegressor(**params)\n",
                "    \n",
                "    model.fit(\n",
                "        X_train, y_train,\n",
                "        eval_set=[(X_val, y_val)],\n",
                "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
                "    )\n",
                "    \n",
                "    val_pred = model.predict(X_val)\n",
                "    mae = mean_absolute_error(y_val, val_pred)\n",
                "    results.append(mae)\n",
                "    print(f\"Fold {fold+1} MAE: {mae:.4f}\")\n",
                "    \n",
                "    final_preds += model.predict(X_test) / 5\n",
                "\n",
                "avg_mae = np.mean(results)\n",
                "print(f\"\\nAverage CV MAE: {avg_mae:.4f}\")\n",
                "\n",
                "# Create Submission\n",
                "sub = pd.DataFrame({'id': test_raw['id'], 'Tm': final_preds})\n",
                "sub.to_csv('../submissions/submission_external_data_lgbm.csv', index=False)\n",
                "print(\"Saved MEGA submission to submissions/submission_external_data_lgbm.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}