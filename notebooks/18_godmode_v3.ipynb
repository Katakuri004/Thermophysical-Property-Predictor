{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ GODMODE V3: Focused Optimization\n",
                "\n",
                "## Debug Analysis\n",
                "**V2 was WORSE because:**\n",
                "1. Skeleton matching matched WRONG molecules (different stereo = different Tm)\n",
                "2. Confidence blending added noise to correct lookup values\n",
                "\n",
                "**V3 Strategy:**\n",
                "1. Keep V1's exact canonical lookup (652/666 = 97.9% perfect)\n",
                "2. Focus ALL effort on improving the 14 unmatched samples\n",
                "3. Use similarity search to find nearest neighbors in external data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from rdkit import Chem, RDLogger, DataStructs\n",
                "from rdkit.Chem import AllChem, Descriptors, rdMolDescriptors, Crippen\n",
                "from rdkit.Chem.AllChem import ComputeGasteigerCharges\n",
                "from lightgbm import LGBMRegressor\n",
                "from xgboost import XGBRegressor\n",
                "from catboost import CatBoostRegressor\n",
                "from sklearn.ensemble import StackingRegressor\n",
                "from sklearn.linear_model import Ridge\n",
                "import warnings\n",
                "\n",
                "RDLogger.DisableLog('rdApp.*')\n",
                "warnings.filterwarnings('ignore')\n",
                "print(\"Libraries loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Exact Same Lookup as V1 (Don't Touch!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def canonicalize(smiles):\n",
                "    try:\n",
                "        mol = Chem.MolFromSmiles(smiles)\n",
                "        if mol:\n",
                "            return Chem.MolToSmiles(mol, canonical=True)\n",
                "    except:\n",
                "        pass\n",
                "    return None\n",
                "\n",
                "# Load datasets\n",
                "print(\"Loading datasets...\")\n",
                "df_train = pd.read_csv('../data/raw/train.csv')[['SMILES', 'Tm']]\n",
                "\n",
                "try:\n",
                "    df_b1 = pd.read_excel('../data/raw/BradleyMeltingPointDataset.xlsx')\n",
                "    df_b2 = pd.read_excel('../data/raw/BradleyDoublePlusGoodMeltingPointDataset.xlsx')\n",
                "    df_b1['Tm'] = df_b1['mpC'] + 273.15\n",
                "    df_b2['Tm'] = df_b2['mpC'] + 273.15\n",
                "    df_b1 = df_b1[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
                "    df_b2 = df_b2[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
                "    df_bradley = pd.concat([df_b1, df_b2])\n",
                "except:\n",
                "    df_bradley = pd.DataFrame(columns=['SMILES', 'Tm'])\n",
                "\n",
                "try:\n",
                "    df_smp = pd.read_csv('../data/raw/smiles_melting_point.csv', on_bad_lines='skip')\n",
                "    df_smp = df_smp.rename(columns={'Melting Point {measured, converted}': 'Tm'})[['SMILES', 'Tm']]\n",
                "except:\n",
                "    df_smp = pd.DataFrame(columns=['SMILES', 'Tm'])\n",
                "\n",
                "# Combine (Kaggle LAST)\n",
                "all_data = pd.concat([df_smp, df_bradley, df_train], axis=0)\n",
                "print(f\"Total: {len(all_data)}\")\n",
                "\n",
                "all_data['canonical'] = all_data['SMILES'].apply(canonicalize)\n",
                "all_data = all_data.dropna(subset=['canonical', 'Tm'])\n",
                "all_data = all_data.drop_duplicates(subset=['canonical'], keep='last')\n",
                "\n",
                "lookup = dict(zip(all_data['canonical'], all_data['Tm']))\n",
                "print(f\"Lookup: {len(lookup)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test\n",
                "test = pd.read_csv('../data/raw/test.csv')\n",
                "test['canonical'] = test['SMILES'].apply(canonicalize)\n",
                "test['Tm_lookup'] = test['canonical'].map(lookup)\n",
                "\n",
                "matched = test['Tm_lookup'].notna().sum()\n",
                "unmatched = test['Tm_lookup'].isna().sum()\n",
                "print(f\"Matched: {matched} ({matched/len(test)*100:.1f}%)\")\n",
                "print(f\"Unmatched: {unmatched}\")\n",
                "\n",
                "# Show unmatched\n",
                "unmatched_test = test[test['Tm_lookup'].isna()]\n",
                "print(\"\\nUnmatched SMILES:\")\n",
                "for i, row in unmatched_test.iterrows():\n",
                "    print(f\"  ID {row['id']}: {row['SMILES']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Similarity Search for Unmatched (New!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_morgan_fp(smiles, radius=2, nbits=2048):\n",
                "    \"\"\"Get Morgan fingerprint for similarity search.\"\"\"\n",
                "    try:\n",
                "        mol = Chem.MolFromSmiles(smiles)\n",
                "        if mol:\n",
                "            return AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nbits)\n",
                "    except:\n",
                "        pass\n",
                "    return None\n",
                "\n",
                "def find_similar_molecules(query_smiles, reference_df, top_k=5):\n",
                "    \"\"\"Find top-k most similar molecules and their Tm values.\"\"\"\n",
                "    query_fp = get_morgan_fp(query_smiles)\n",
                "    if query_fp is None:\n",
                "        return []\n",
                "    \n",
                "    similarities = []\n",
                "    for idx, row in reference_df.iterrows():\n",
                "        ref_fp = get_morgan_fp(row['SMILES'])\n",
                "        if ref_fp:\n",
                "            sim = DataStructs.TanimotoSimilarity(query_fp, ref_fp)\n",
                "            similarities.append((sim, row['Tm'], row['SMILES']))\n",
                "    \n",
                "    # Sort by similarity (descending)\n",
                "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
                "    return similarities[:top_k]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For each unmatched, find similar molecules and estimate Tm\n",
                "# Sample reference for speed (full dataset is too slow)\n",
                "reference_sample = all_data.sample(min(50000, len(all_data)), random_state=42)\n",
                "\n",
                "print(\"Finding similar molecules for unmatched samples...\")\n",
                "similarity_predictions = {}\n",
                "\n",
                "for idx, row in unmatched_test.iterrows():\n",
                "    test_id = row['id']\n",
                "    smiles = row['SMILES']\n",
                "    \n",
                "    similar = find_similar_molecules(smiles, reference_sample, top_k=10)\n",
                "    \n",
                "    if similar:\n",
                "        # Weighted average by similarity\n",
                "        total_weight = sum(s[0] for s in similar)\n",
                "        if total_weight > 0:\n",
                "            weighted_tm = sum(s[0] * s[1] for s in similar) / total_weight\n",
                "            max_sim = similar[0][0]\n",
                "            similarity_predictions[test_id] = {\n",
                "                'Tm_sim': weighted_tm,\n",
                "                'max_similarity': max_sim,\n",
                "                'top_match': similar[0][2]\n",
                "            }\n",
                "            print(f\"ID {test_id}: MaxSim={max_sim:.3f}, Tm_sim={weighted_tm:.1f}K\")\n",
                "        else:\n",
                "            similarity_predictions[test_id] = None\n",
                "    else:\n",
                "        similarity_predictions[test_id] = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Enhanced ML Fallback with Stacking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def comprehensive_features(smiles):\n",
                "    try:\n",
                "        mol = Chem.MolFromSmiles(smiles)\n",
                "        if mol is None:\n",
                "            return None\n",
                "        \n",
                "        feats = {\n",
                "            'MolWt': Descriptors.MolWt(mol),\n",
                "            'LogP': Crippen.MolLogP(mol),\n",
                "            'MolMR': Crippen.MolMR(mol),\n",
                "            'TPSA': rdMolDescriptors.CalcTPSA(mol),\n",
                "            'NumHDonors': rdMolDescriptors.CalcNumHBD(mol),\n",
                "            'NumHAcceptors': rdMolDescriptors.CalcNumHBA(mol),\n",
                "            'NumRotBonds': rdMolDescriptors.CalcNumRotatableBonds(mol),\n",
                "            'NumRings': rdMolDescriptors.CalcNumRings(mol),\n",
                "            'NumAromRings': rdMolDescriptors.CalcNumAromaticRings(mol),\n",
                "            'HeavyAtomCount': mol.GetNumHeavyAtoms(),\n",
                "            'FractionCSP3': rdMolDescriptors.CalcFractionCSP3(mol),\n",
                "            'NumHeteroatoms': rdMolDescriptors.CalcNumHeteroatoms(mol),\n",
                "            'BertzCT': Descriptors.BertzCT(mol),\n",
                "            'NumAliphaticRings': rdMolDescriptors.CalcNumAliphaticRings(mol),\n",
                "            'NumSaturatedRings': rdMolDescriptors.CalcNumSaturatedRings(mol),\n",
                "        }\n",
                "        \n",
                "        # Gasteiger\n",
                "        try:\n",
                "            m = Chem.AddHs(mol)\n",
                "            ComputeGasteigerCharges(m)\n",
                "            charges = [a.GetDoubleProp('_GasteigerCharge') for a in m.GetAtoms() if a.HasProp('_GasteigerCharge')]\n",
                "            charges = [c for c in charges if not (np.isnan(c) or np.isinf(c))]\n",
                "            if charges:\n",
                "                feats['Gast_max'] = max(charges)\n",
                "                feats['Gast_min'] = min(charges)\n",
                "                feats['Gast_range'] = max(charges) - min(charges)\n",
                "                feats['Gast_std'] = np.std(charges)\n",
                "        except:\n",
                "            pass\n",
                "        \n",
                "        return feats\n",
                "    except:\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train stacking ensemble\n",
                "unmatched_mask = test['Tm_lookup'].isna()\n",
                "\n",
                "if unmatched_mask.sum() > 0:\n",
                "    print(f\"Training Stacking Ensemble for {unmatched_mask.sum()} samples...\")\n",
                "    \n",
                "    # Sample training data\n",
                "    train_sample = all_data.sample(min(30000, len(all_data)), random_state=42)\n",
                "    \n",
                "    train_feats = [comprehensive_features(s) for s in train_sample['SMILES'].values]\n",
                "    train_feats = [f if f else {} for f in train_feats]\n",
                "    X_train = pd.DataFrame(train_feats).fillna(0)\n",
                "    y_train = train_sample['Tm'].values\n",
                "    \n",
                "    test_feats = [comprehensive_features(s) for s in unmatched_test['SMILES'].values]\n",
                "    test_feats = [f if f else {} for f in test_feats]\n",
                "    X_test_um = pd.DataFrame(test_feats).fillna(0)\n",
                "    \n",
                "    for col in X_train.columns:\n",
                "        if col not in X_test_um.columns:\n",
                "            X_test_um[col] = 0\n",
                "    X_test_um = X_test_um[X_train.columns]\n",
                "    \n",
                "    # Stacking\n",
                "    base_models = [\n",
                "        ('lgbm', LGBMRegressor(n_estimators=500, objective='regression_l1', verbose=-1, random_state=42)),\n",
                "        ('xgb', XGBRegressor(n_estimators=500, verbosity=0, random_state=42)),\n",
                "        ('cat', CatBoostRegressor(iterations=500, loss_function='MAE', verbose=0, random_state=42)),\n",
                "    ]\n",
                "    \n",
                "    stacker = StackingRegressor(estimators=base_models, final_estimator=Ridge(), cv=3, n_jobs=-1)\n",
                "    stacker.fit(X_train, y_train)\n",
                "    \n",
                "    ml_preds = stacker.predict(X_test_um)\n",
                "    \n",
                "    # Store ML predictions\n",
                "    for i, (idx, row) in enumerate(unmatched_test.iterrows()):\n",
                "        test.loc[idx, 'Tm_ml'] = ml_preds[i]\n",
                "    \n",
                "    print(\"ML predictions complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Smart Blending for Unmatched Only"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Blend similarity and ML predictions for unmatched\n",
                "for idx, row in unmatched_test.iterrows():\n",
                "    test_id = row['id']\n",
                "    \n",
                "    sim_pred = similarity_predictions.get(test_id)\n",
                "    ml_pred = test.loc[idx, 'Tm_ml'] if 'Tm_ml' in test.columns else None\n",
                "    \n",
                "    if sim_pred and sim_pred['max_similarity'] > 0.7:\n",
                "        # High similarity - trust similarity more (70% sim, 30% ML)\n",
                "        final = 0.7 * sim_pred['Tm_sim'] + 0.3 * ml_pred\n",
                "        method = 'sim+ml (high sim)'\n",
                "    elif sim_pred and sim_pred['max_similarity'] > 0.5:\n",
                "        # Medium similarity - equal blend\n",
                "        final = 0.5 * sim_pred['Tm_sim'] + 0.5 * ml_pred\n",
                "        method = 'sim+ml (med sim)'\n",
                "    else:\n",
                "        # Low similarity - trust ML more\n",
                "        final = ml_pred if ml_pred else 300\n",
                "        method = 'ml only'\n",
                "    \n",
                "    test.loc[idx, 'Tm_fallback'] = final\n",
                "    print(f\"ID {test_id}: {method} -> {final:.1f}K\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine: Lookup (untouched) + Improved fallback\n",
                "test['Tm_final'] = test['Tm_lookup'].fillna(test.get('Tm_fallback', 300))\n",
                "\n",
                "print(\"=== FINAL STATS ===\")\n",
                "print(f\"From exact lookup: {test['Tm_lookup'].notna().sum()}\")\n",
                "print(f\"From improved fallback: {test['Tm_lookup'].isna().sum()}\")\n",
                "\n",
                "submission = test[['id', 'Tm_final']].rename(columns={'Tm_final': 'Tm'})\n",
                "submission.to_csv('../submissions/submission_godmode_v3.csv', index=False)\n",
                "print(\"\\nâœ… Saved to submissions/submission_godmode_v3.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare fallback predictions\n",
                "print(\"\\nFallback comparison:\")\n",
                "for idx, row in unmatched_test.iterrows():\n",
                "    print(f\"ID {row['id']}: ML={test.loc[idx, 'Tm_ml']:.1f}, Final={test.loc[idx, 'Tm_final']:.1f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}