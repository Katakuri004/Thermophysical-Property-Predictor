{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CNU Validation: Calibrated Neighborhood Uncertainty\n",
                "\n",
                "Validates the theoretical framework:\n",
                "1. Monotonicity of u(x) - does uncertainty score rank risk correctly?\n",
                "2. Per-regime coverage - do intervals achieve target coverage?\n",
                "3. Ablation - contribution of each uncertainty primitive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "\n",
                "from src.calibration import NeighborhoodUncertainty, CNUCalibrator, NeighborhoodFeatures\n",
                "\n",
                "# Settings\n",
                "plt.rcParams['figure.dpi'] = 150\n",
                "FIG_DIR = Path('../figures/paper')\n",
                "FIG_DIR.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load train dataset\n",
                "train_df = pd.read_csv('../data/raw/train.csv')\n",
                "print(f\"Train columns: {train_df.columns.tolist()}\")\n",
                "print(f\"Train shape: {train_df.shape}\")\n",
                "\n",
                "# Get column names\n",
                "smiles_col = [c for c in train_df.columns if 'smiles' in c.lower()][0]\n",
                "tm_col = [c for c in train_df.columns if 'tm' in c.lower() or 'melt' in c.lower()][0]\n",
                "print(f\"Using SMILES column: {smiles_col}, Tm column: {tm_col}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load SMP data with proper handling\n",
                "smp_df = pd.read_csv('../data/raw/smiles_melting_point.csv')\n",
                "print(f\"SMP columns: {smp_df.columns.tolist()}\")\n",
                "print(f\"SMP shape: {smp_df.shape}\")\n",
                "\n",
                "# Find columns\n",
                "smp_smiles_col = [c for c in smp_df.columns if 'smiles' in c.lower()][0]\n",
                "smp_tm_cols = [c for c in smp_df.columns if 'melt' in c.lower() or 'point' in c.lower()]\n",
                "smp_tm_col = smp_tm_cols[0] if smp_tm_cols else None\n",
                "print(f\"SMP SMILES column: {smp_smiles_col}, Tm column: {smp_tm_col}\")\n",
                "\n",
                "# Convert Tm to numeric\n",
                "if smp_tm_col:\n",
                "    smp_df[smp_tm_col] = pd.to_numeric(smp_df[smp_tm_col], errors='coerce')\n",
                "    smp_df = smp_df.dropna(subset=[smp_tm_col, smp_smiles_col])\n",
                "    print(f\"SMP after cleaning: {len(smp_df)} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Bradley data\n",
                "try:\n",
                "    bradley_df = pd.read_excel('../data/raw/BradleyMeltingPointDataset.xlsx')\n",
                "    print(f\"Bradley columns: {bradley_df.columns.tolist()}\")\n",
                "    \n",
                "    # Find columns\n",
                "    brad_smiles_col = [c for c in bradley_df.columns if 'smiles' in c.lower()][0]\n",
                "    brad_tm_col = [c for c in bradley_df.columns if 'mpc' in c.lower() or 'mp' in c.lower()][0]\n",
                "    bradley_df[brad_tm_col] = pd.to_numeric(bradley_df[brad_tm_col], errors='coerce')\n",
                "    bradley_df = bradley_df.dropna(subset=[brad_tm_col, brad_smiles_col])\n",
                "    print(f\"Bradley after cleaning: {len(bradley_df)} rows\")\n",
                "except Exception as e:\n",
                "    print(f\"Could not load Bradley: {e}\")\n",
                "    bradley_df = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine datasets\n",
                "all_smiles = list(train_df[smiles_col])\n",
                "all_tms = train_df[tm_col].values.astype(float).copy()\n",
                "\n",
                "# Add SMP\n",
                "if smp_tm_col:\n",
                "    smp_tms = smp_df[smp_tm_col].values.astype(float)\n",
                "    # Check if Celsius (values < 200 on average)\n",
                "    if np.nanmean(smp_tms) < 200:\n",
                "        smp_tms = smp_tms + 273.15\n",
                "    all_smiles.extend(list(smp_df[smp_smiles_col]))\n",
                "    all_tms = np.concatenate([all_tms, smp_tms])\n",
                "    print(f\"Added {len(smp_df)} SMP molecules\")\n",
                "\n",
                "# Add Bradley\n",
                "if bradley_df is not None and len(bradley_df) > 0:\n",
                "    brad_tms = bradley_df[brad_tm_col].values.astype(float)\n",
                "    if np.nanmean(brad_tms) < 200:\n",
                "        brad_tms = brad_tms + 273.15\n",
                "    all_smiles.extend(list(bradley_df[brad_smiles_col]))\n",
                "    all_tms = np.concatenate([all_tms, brad_tms])\n",
                "    print(f\"Added {len(bradley_df)} Bradley molecules\")\n",
                "\n",
                "print(f\"Total: {len(all_smiles)} molecules\")\n",
                "\n",
                "# Clean NaN values\n",
                "valid_mask = ~np.isnan(all_tms)\n",
                "all_smiles = [s for s, v in zip(all_smiles, valid_mask) if v]\n",
                "all_tms = all_tms[valid_mask]\n",
                "print(f\"After NaN removal: {len(all_smiles)} molecules\")\n",
                "\n",
                "# Split 90/10\n",
                "n = len(all_smiles)\n",
                "np.random.seed(42)\n",
                "perm = np.random.permutation(n)\n",
                "n_train = int(0.9 * n)\n",
                "\n",
                "train_idx, calib_idx = perm[:n_train], perm[n_train:]\n",
                "train_smiles = [all_smiles[i] for i in train_idx]\n",
                "train_tms = all_tms[train_idx]\n",
                "calib_smiles = [all_smiles[i] for i in calib_idx]\n",
                "calib_tms = all_tms[calib_idx]\n",
                "\n",
                "print(f\"\\nTrain: {len(train_smiles)}, Calib: {len(calib_smiles)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Build Index and Calibrate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.hierarchical_mp_v8 import HierarchicalMPPredictorV8\n",
                "\n",
                "predictor = HierarchicalMPPredictorV8(n_regimes=5, alpha=0.10)\n",
                "predictor.fit_index(train_smiles, train_tms)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calibrate CNU\n",
                "result = predictor.fit_calibration(calib_smiles, calib_tms)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Monotonicity Validation\n",
                "\n",
                "**Lemma (Risk Ranking)**: If u(x) is monotone in each primitive uncertainty source and empirical residual quantiles increase with u(x), then u(x) is a valid risk ranking statistic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate monotonicity\n",
                "mono_df = predictor.validate_monotonicity(calib_smiles, calib_tms)\n",
                "print(mono_df)\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.bar(mono_df['decile'], mono_df['mae'], color='steelblue', edgecolor='black')\n",
                "ax.set_xlabel('Uncertainty Score u(x) Decile', fontsize=12)\n",
                "ax.set_ylabel('Mean Absolute Error (K)', fontsize=12)\n",
                "ax.set_title('Monotonicity Check: MAE vs. u(x) Decile', fontsize=14)\n",
                "ax.set_xticks(range(1, 11))\n",
                "\n",
                "# Add trend line\n",
                "z = np.polyfit(mono_df['decile'], mono_df['mae'], 1)\n",
                "p = np.poly1d(z)\n",
                "ax.plot(mono_df['decile'], p(mono_df['decile']), 'r--', linewidth=2, label=f'Trend: slope={z[0]:.1f}')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIG_DIR / 'fig_monotonicity.png', dpi=300, bbox_inches='tight')\n",
                "plt.savefig(FIG_DIR / 'fig_monotonicity.pdf', bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Check monotonicity\n",
                "is_monotone = all(mono_df['mae'].diff().dropna() >= -5)  # Allow small noise\n",
                "print(f\"\\nMonotonicity check: {'PASSED' if is_monotone else 'FAILED'}\")\n",
                "print(f\"MAE range: {mono_df['mae'].min():.1f}K to {mono_df['mae'].max():.1f}K\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ablation Study\n",
                "\n",
                "Contribution of each uncertainty primitive: (1-s₁), σ_w, 1/k_eff, ambiguity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ablation\n",
                "ablation_df = predictor.compute_ablation(calib_smiles, calib_tms)\n",
                "print(ablation_df)\n",
                "\n",
                "# Plot\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Mean width\n",
                "ax1.barh(ablation_df['config'], ablation_df['mean_width'], color='steelblue')\n",
                "ax1.set_xlabel('Mean Interval Width (K)', fontsize=12)\n",
                "ax1.set_title('Interval Width by Configuration', fontsize=14)\n",
                "ax1.invert_yaxis()\n",
                "\n",
                "# Coverage\n",
                "ax2.barh(ablation_df['config'], ablation_df['coverage'], color='seagreen')\n",
                "ax2.axvline(0.90, color='red', linestyle='--', label='Target 90%')\n",
                "ax2.set_xlabel('Empirical Coverage', fontsize=12)\n",
                "ax2.set_title('Coverage by Configuration', fontsize=14)\n",
                "ax2.invert_yaxis()\n",
                "ax2.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIG_DIR / 'fig_ablation.png', dpi=300, bbox_inches='tight')\n",
                "plt.savefig(FIG_DIR / 'fig_ablation.pdf', bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Per-Regime Coverage\n",
                "\n",
                "**Theorem 2**: Regime-conditional coverage ≥ 1-α"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Per-regime analysis\n",
                "regime_data = []\n",
                "for regime, q in result.regime_quantiles.items():\n",
                "    n = result.regime_counts.get(regime, 0)\n",
                "    cov = result.coverage_achieved.get(regime, 0)\n",
                "    regime_data.append({'regime': regime, 'quantile': q, 'n': n, 'coverage': cov})\n",
                "\n",
                "regime_df = pd.DataFrame(regime_data).sort_values('regime')\n",
                "print(regime_df)\n",
                "\n",
                "# Plot\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Quantiles\n",
                "ax1.bar(regime_df['regime'], regime_df['quantile'], color='steelblue')\n",
                "ax1.set_xlabel('Regime', fontsize=12)\n",
                "ax1.set_ylabel('Conformal Quantile (K)', fontsize=12)\n",
                "ax1.set_title('Per-Regime Error Quantiles', fontsize=14)\n",
                "ax1.tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Coverage\n",
                "colors = ['green' if c >= 0.88 else 'orange' if c >= 0.85 else 'red' for c in regime_df['coverage']]\n",
                "ax2.bar(regime_df['regime'], regime_df['coverage'], color=colors)\n",
                "ax2.axhline(0.90, color='red', linestyle='--', label='Target 90%')\n",
                "ax2.set_xlabel('Regime', fontsize=12)\n",
                "ax2.set_ylabel('Empirical Coverage', fontsize=12)\n",
                "ax2.set_title('Per-Regime Coverage', fontsize=14)\n",
                "ax2.tick_params(axis='x', rotation=45)\n",
                "ax2.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIG_DIR / 'fig_regime_coverage.png', dpi=300, bbox_inches='tight')\n",
                "plt.savefig(FIG_DIR / 'fig_regime_coverage.pdf', bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Learned Weights Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Learned weights\n",
                "weights = result.weights\n",
                "primitives = ['1 - s₁\\n(coverage)', 'σ_w\\n(disagreement)', '1/k_eff\\n(sparsity)', 'ambiguity\\n(gap)']\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(8, 5))\n",
                "bars = ax.bar(primitives, weights, color='steelblue', edgecolor='black')\n",
                "ax.set_ylabel('Learned Weight (w ≥ 0)', fontsize=12)\n",
                "ax.set_title('Learned Uncertainty Weights via NNLS', fontsize=14)\n",
                "\n",
                "for i, w in enumerate(weights):\n",
                "    ax.annotate(f'{w:.3f}', (i, w + 0.01), ha='center', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIG_DIR / 'fig_learned_weights.png', dpi=300, bbox_inches='tight')\n",
                "plt.savefig(FIG_DIR / 'fig_learned_weights.pdf', bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"Weights: {dict(zip(['w_cov', 'w_var', 'w_sparse', 'w_ambig'], weights))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"CNU VALIDATION SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nLearned weights: {weights}\")\n",
                "print(f\"Global quantile: {result.global_quantile:.1f}K\")\n",
                "print(f\"\\nMonotonicity: {'PASSED' if is_monotone else 'FAILED'}\")\n",
                "print(f\"  MAE in lowest u(x) decile: {mono_df['mae'].iloc[0]:.1f}K\")\n",
                "print(f\"  MAE in highest u(x) decile: {mono_df['mae'].iloc[-1]:.1f}K\")\n",
                "print(f\"\\nPer-regime coverage:\")\n",
                "for _, row in regime_df.iterrows():\n",
                "    status = '✓' if row['coverage'] >= 0.88 else '⚠'\n",
                "    print(f\"  {status} {row['regime']}: {row['coverage']:.1%} (n={row['n']})\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}